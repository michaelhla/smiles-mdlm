2024-12-07 21:45:06,362 INFO    MainThread:72508 [wandb_setup.py:_flush():68] Configure stats pid to 72508
2024-12-07 21:45:06,362 INFO    MainThread:72508 [wandb_setup.py:_flush():68] Loading settings from /root/.config/wandb/settings
2024-12-07 21:45:06,362 INFO    MainThread:72508 [wandb_setup.py:_flush():68] Loading settings from /root/smiles-mdlm/outputs/chebi/2024.12.07/214452/wandb/settings
2024-12-07 21:45:06,362 INFO    MainThread:72508 [wandb_setup.py:_flush():68] Loading settings from environment variables: {'_require_service': 'True'}
2024-12-07 21:45:06,362 INFO    MainThread:72508 [wandb_setup.py:_flush():68] Inferring run settings from compute environment: {'program_relpath': 'main.py', 'program': '/root/smiles-mdlm/main.py'}
2024-12-07 21:45:06,362 INFO    MainThread:72508 [wandb_init.py:_log_setup():476] Logging user logs to ./wandb/run-20241207_214506-mdlm-chebi_1/logs/debug.log
2024-12-07 21:45:06,363 INFO    MainThread:72508 [wandb_init.py:_log_setup():477] Logging internal logs to ./wandb/run-20241207_214506-mdlm-chebi_1/logs/debug-internal.log
2024-12-07 21:45:06,363 INFO    MainThread:72508 [wandb_init.py:init():516] calling init triggers
2024-12-07 21:45:06,363 INFO    MainThread:72508 [wandb_init.py:init():519] wandb.init called with sweep_config: {}
config: {'mode': 'train', 'diffusion': 'absorbing_state', 'backbone': 'dit', 'parameterization': 'subs', 'time_conditioning': False, 'T': 0, 'subs_masking': False, 'seed': 1, 'loader': {'global_batch_size': 512, 'eval_global_batch_size': 512, 'batch_size': 512, 'eval_batch_size': 512, 'num_workers': 128, 'pin_memory': True}, 'sampling': {'predictor': 'ddpm_cache', 'steps': 1000, 'noise_removal': True, 'num_sample_batches': 2, 'num_sample_log': 2, 'semi_ar': False, 'stride_length': 1, 'num_strides': 1}, 'training': {'ema': 0.9999, 'antithetic_sampling': True, 'importance_sampling': False, 'sampling_eps': 0.001, 'change_of_variables': False, 'max_epochs': 100}, 'eval': {'checkpoint_path': '', 'disable_ema': False, 'compute_generative_perplexity': False, 'perplexity_batch_size': 8, 'compute_perplexity_on_sanity': False, 'gen_ppl_eval_model_name_or_path': 'gpt2-large', 'generate_samples': True}, 'optim': {'weight_decay': 0, 'lr': 0.0003, 'beta1': 0.9, 'beta2': 0.999, 'eps': 1e-08}, 'trainer': {'_target_': 'lightning.Trainer', 'accelerator': 'cuda', 'num_nodes': 1, 'devices': 1, 'accumulate_grad_batches': 1, 'gradient_clip_val': 1.0, 'precision': 'bf16', 'num_sanity_val_steps': 2, 'max_steps': 1000000, 'log_every_n_steps': 10, 'limit_train_batches': 1.0, 'limit_val_batches': 1.0, 'val_check_interval': 10000}, 'wandb': {'project': 'text-diffusion', 'notes': 'Mulan for text', 'group': None, 'job_type': None, 'name': 'mdlm-chebi', 'id': 'mdlm-chebi_1', 'tags': ['loglinear', 'chebi', 'chebi']}, 'checkpointing': {'save_dir': '/root/smiles-mdlm/outputs/chebi/2024.12.07/214452', 'resume_from_ckpt': True, 'resume_ckpt_path': '/root/smiles-mdlm/outputs/chebi/2024.12.07/214452/checkpoints/last.ckpt'}, 'callbacks': {'checkpoint_every_n_steps': {'_target_': 'lightning.pytorch.callbacks.ModelCheckpoint', 'save_top_k': -1, 'save_last': True, 'dirpath': '/root/smiles-mdlm/outputs/chebi/2024.12.07/214452/checkpoints', 'verbose': True, 'auto_insert_metric_name': False, 'every_n_train_steps': 500}, 'checkpoint_monitor': {'_target_': 'lightning.pytorch.callbacks.ModelCheckpoint', 'monitor': 'val/nll', 'mode': 'min', 'save_top_k': 1, 'save_last': False, 'dirpath': '/root/smiles-mdlm/outputs/chebi/2024.12.07/214452/checkpoints', 'filename': 'best', 'auto_insert_metric_name': False, 'verbose': True}, 'learning_rate_monitor': {'_target_': 'lightning.pytorch.callbacks.LearningRateMonitor', 'logging_interval': 'step'}}, 'data': {'train': 'chebi', 'valid': 'chebi', 'tokenizer_name_or_path': 'bert-base-uncased', 'smiles_tokenizer_type': 'custom', 'cache_dir': './cache', 'wrap': True, 'streaming': False, 'model': {'smiles_length': 256, 'text_length': 512}}, 'model': {'_target_': 'models.dit.DIT', 'hidden_size': 256, 'n_heads': 12, 'n_blocks': 12, 'dropout': 0.2, 'cond_dim': 768, 'length': 128, 'scale_by_sigma': True, 'text_conditioning': True}, 'strategy': {'_target_': 'lightning.pytorch.strategies.DDPStrategy', 'find_unused_parameters': False}, 'noise': {'type': 'loglinear', 'sigma_min': 0.0001, 'sigma_max': 20}, 'lr_scheduler': {'_target_': 'transformers.get_constant_schedule_with_warmup', 'num_warmup_steps': 2500}}
2024-12-07 21:45:06,363 INFO    MainThread:72508 [wandb_init.py:init():569] starting backend
2024-12-07 21:45:06,363 INFO    MainThread:72508 [wandb_init.py:init():573] setting up manager
2024-12-07 21:45:06,366 INFO    MainThread:72508 [backend.py:_multiprocessing_setup():102] multiprocessing start_methods=fork,spawn,forkserver, using: spawn
2024-12-07 21:45:06,369 INFO    MainThread:72508 [wandb_init.py:init():580] backend started and connected
2024-12-07 21:45:06,377 INFO    MainThread:72508 [wandb_init.py:init():658] updated telemetry
2024-12-07 21:45:06,497 INFO    MainThread:72508 [wandb_init.py:init():693] communicating run to backend with 60 second timeout
2024-12-07 21:45:07,530 INFO    MainThread:72508 [wandb_init.py:init():722] run resumed
2024-12-07 21:45:07,534 INFO    MainThread:72508 [wandb_run.py:_on_init():2000] communicating current version
2024-12-07 21:45:07,779 INFO    MainThread:72508 [wandb_run.py:_on_init():2004] got version response upgrade_message: "wandb version 0.19.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"

2024-12-07 21:45:07,779 INFO    MainThread:72508 [wandb_init.py:init():728] starting run threads in backend
2024-12-07 21:45:07,834 INFO    MainThread:72508 [wandb_run.py:_console_start():1980] atexit reg
2024-12-07 21:45:07,834 INFO    MainThread:72508 [wandb_run.py:_redirect():1838] redirect: SettingsConsole.WRAP_RAW
2024-12-07 21:45:07,835 INFO    MainThread:72508 [wandb_run.py:_redirect():1903] Wrapping output streams.
2024-12-07 21:45:07,835 INFO    MainThread:72508 [wandb_run.py:_redirect():1925] Redirects installed.
2024-12-07 21:45:07,836 INFO    MainThread:72508 [wandb_init.py:init():765] run started, returning control to user process
2024-12-07 21:45:08,063 INFO    MainThread:72508 [wandb_run.py:_config_callback():1160] config_cb None None {'config': '{\'mode\': \'train\', \'diffusion\': \'absorbing_state\', \'backbone\': \'dit\', \'parameterization\': \'subs\', \'time_conditioning\': False, \'T\': 0, \'subs_masking\': False, \'seed\': 1, \'loader\': {\'global_batch_size\': 512, \'eval_global_batch_size\': \'${.global_batch_size}\', \'batch_size\': \'${div_up:${.global_batch_size}, ${eval:${trainer.devices} * ${trainer.num_nodes}}}\', \'eval_batch_size\': \'${div_up:${.eval_global_batch_size}, ${eval:${trainer.devices} * ${trainer.num_nodes}}}\', \'num_workers\': \'${eval:"len(__import__(\\\'os\\\').sched_getaffinity(0))"}\', \'pin_memory\': True}, \'sampling\': {\'predictor\': \'ddpm_cache\', \'steps\': 1000, \'noise_removal\': True, \'num_sample_batches\': 2, \'num_sample_log\': 2, \'semi_ar\': False, \'stride_length\': 1, \'num_strides\': 1}, \'training\': {\'ema\': 0.9999, \'antithetic_sampling\': True, \'importance_sampling\': False, \'sampling_eps\': 0.001, \'change_of_variables\': False, \'max_epochs\': 100}, \'eval\': {\'checkpoint_path\': \'\', \'disable_ema\': False, \'compute_generative_perplexity\': False, \'perplexity_batch_size\': 8, \'compute_perplexity_on_sanity\': False, \'gen_ppl_eval_model_name_or_path\': \'gpt2-large\', \'generate_samples\': True}, \'optim\': {\'weight_decay\': 0, \'lr\': 0.0003, \'beta1\': 0.9, \'beta2\': 0.999, \'eps\': 1e-08}, \'trainer\': {\'_target_\': \'lightning.Trainer\', \'accelerator\': \'cuda\', \'num_nodes\': 1, \'devices\': \'${device_count:}\', \'accumulate_grad_batches\': \'${div_up:${loader.global_batch_size}, ${eval:${trainer.devices} * ${loader.batch_size} * ${trainer.num_nodes}}}\', \'gradient_clip_val\': 1.0, \'precision\': \'bf16\', \'num_sanity_val_steps\': 2, \'max_steps\': 1000000, \'log_every_n_steps\': 10, \'limit_train_batches\': 1.0, \'limit_val_batches\': 1.0, \'val_check_interval\': 10000}, \'wandb\': {\'project\': \'text-diffusion\', \'notes\': \'Mulan for text\', \'group\': None, \'job_type\': None, \'name\': \'mdlm-chebi\', \'id\': \'${.name}_${seed}\', \'tags\': [\'${noise.type}\', \'${data.train}\', \'${data.valid}\']}, \'checkpointing\': {\'save_dir\': \'${cwd:}\', \'resume_from_ckpt\': True, \'resume_ckpt_path\': \'${.save_dir}/checkpoints/last.ckpt\'}, \'callbacks\': {\'checkpoint_every_n_steps\': {\'_target_\': \'lightning.pytorch.callbacks.ModelCheckpoint\', \'save_top_k\': -1, \'save_last\': True, \'dirpath\': \'${checkpointing.save_dir}/checkpoints\', \'verbose\': True, \'auto_insert_metric_name\': False, \'every_n_train_steps\': 500}, \'checkpoint_monitor\': {\'_target_\': \'lightning.pytorch.callbacks.ModelCheckpoint\', \'monitor\': \'val/nll\', \'mode\': \'min\', \'save_top_k\': 1, \'save_last\': False, \'dirpath\': \'${checkpointing.save_dir}/checkpoints\', \'filename\': \'best\', \'auto_insert_metric_name\': False, \'verbose\': True}, \'learning_rate_monitor\': {\'_target_\': \'lightning.pytorch.callbacks.LearningRateMonitor\', \'logging_interval\': \'step\'}}, \'data\': {\'train\': \'chebi\', \'valid\': \'chebi\', \'tokenizer_name_or_path\': \'bert-base-uncased\', \'smiles_tokenizer_type\': \'custom\', \'cache_dir\': \'./cache\', \'wrap\': True, \'streaming\': False, \'model\': {\'smiles_length\': 256, \'text_length\': 512}}, \'model\': {\'_target_\': \'models.dit.DIT\', \'hidden_size\': 256, \'n_heads\': 12, \'n_blocks\': 12, \'dropout\': 0.2, \'cond_dim\': 768, \'length\': 128, \'scale_by_sigma\': True, \'text_conditioning\': True}, \'strategy\': {\'_target_\': \'lightning.pytorch.strategies.DDPStrategy\', \'find_unused_parameters\': False}, \'noise\': {\'type\': \'loglinear\', \'sigma_min\': 0.0001, \'sigma_max\': 20}, \'lr_scheduler\': {\'_target_\': \'transformers.get_constant_schedule_with_warmup\', \'num_warmup_steps\': 2500}}', 'tokenizer': None}
2024-12-07 21:45:41,413 WARNING MsgRouterThr:72508 [router.py:message_loop():77] message_loop has been closed
